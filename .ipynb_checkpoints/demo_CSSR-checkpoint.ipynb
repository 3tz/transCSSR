{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Demonstration of Estimating $\\epsilon$-machines using <tt>CSSR</tt> and Analyzing Their Information-Theoretic Properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line just makes plots visible in-notebook, rather than opening stand-alone windows.\n",
    "\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in the required Python packages. In addition to the necessary packages listed on the GitHub page for <tt>transCSSR</tt>, you will also need [graphviz](http://graphviz.readthedocs.io/en/stable/manual.html) to visualize the $\\epsilon$-machines in-notebook, and [scikit-learn](http://scikit-learn.org/stable/) to compute the log-loss for selecting the appropriate lookback length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transCSSR_bc import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import graphviz\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the $\\epsilon$-Machine Using <tt>CSSR</tt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load in the data files. <tt>transCSSR</tt> was originally designed for inferring $\\epsilon$-transducers, so to infer an $\\epsilon$-machine, we fix the input stream as a constant. This is equivalent to the <tt>CSSR</tt> algorithm, which you can learn more about [here](http://bactra.org/CSSR/).\n",
    "\n",
    "The data files should be in the data directory, and should have a .dat file suffix. The time series should consist of single character symbols, with no spacing between symbols. See several of the extant files for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yt is the output. Xt should be set to the null string.\n",
    "\n",
    "data_prefix = ''\n",
    "\n",
    "Yt_name = 'even'\n",
    "\n",
    "Xt_name = ''\n",
    "\n",
    "machine_fname = 'transCSSR_results/+.dot'\n",
    "transducer_fname = 'transCSSR_results/+{}.dot'.format(Yt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#\n",
    "# Load in the data for each process.\n",
    "#\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "stringY = open('data/{}{}.dat'.format(data_prefix, Yt_name)).readline().strip()\n",
    "\n",
    "if Xt_name == '':\n",
    "\tstringX = '0'*len(stringY)\n",
    "else:\n",
    "\tstringX = open('data/{}{}.dat'.format(data_prefix, Xt_name)).readline().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set several parameters for the <tt>transCSSR</tt> algorithm.\n",
    "\n",
    "* <tt>axs</tt> / <tt>axs</tt> correspond to the input / output alphabets of your data.\n",
    "\n",
    "* <tt>alpha</tt> specifies the level for the hypothesis test used to split histories with different distributions over their next-step-futures. That is, <tt>alpha</tt> specifies the probability that you would split two histories when you shouldn't (when the histories in fact have the same distribution over their next-step-futures), also known as the probability of a Type I error.\n",
    "\n",
    "* <tt>L_max_words</tt> specifies the largest lookback $L$ to use when estimating $P(X_{L+1} \\mid X_{1}^{L})$. These predictive probabilities will be estimated for all $L \\leq$ <tt>L_max_words</tt>.\n",
    "\n",
    "* <tt>L_max_CSSR</tt> specifies to what depth we considering splitting past histories. This parameter effects the [bias-variance](https://en.wikipedia.org/wiki/Bias–variance_tradeoff) tradeoff. A smaller <tt>L_max_CSSR</tt> will lead to a more stable estimate of the underlying $\\epsilon$-machine (low variance), but may not match the true $\\epsilon$-machine (high bias). A larger <tt>L_max_CSSR</tt> will lead to more highly variable estimates of the $\\epsilon$-machine (high variance), but will more closely approximate the true $\\epsilon$-machine (low bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#\n",
    "# Set the parameters and associated quantities:\n",
    "# \taxs, ays -- the input / output alphabets\n",
    "# \talpha    -- the significance level associated with\n",
    "# \t            CSSR's hypothesis tests.\n",
    "# \tL        -- The maximum history length to look\n",
    "#               back when inferring predictive\n",
    "#               distributions.\n",
    "#\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "axs = ['0']\n",
    "ays = ['0', '1']\n",
    "\n",
    "e_symbols = list(itertools.product(axs, ays)) # All of the possible pairs of emission\n",
    "                                              # symbols for (x, y)\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "verbose = False\n",
    "\n",
    "# L is the maximum amount we want to ever look back.\n",
    "\n",
    "L_max_words = 5\n",
    "L_max_CSSR  = 5\n",
    "\n",
    "assert L_max_CSSR <= L_max_words, \"L_max_CSSR must be less than or equal to L_max_words\"\n",
    "\n",
    "inf_alg = 'transCSSR'\n",
    "\n",
    "Tx = len(stringX); Ty = len(stringY)\n",
    "\n",
    "assert Tx == Ty, 'The two time series must have the same length.'\n",
    "\n",
    "T = Tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first to estimate the predictive distributions over all words using <tt>estimate_predictive_distributions</tt>, and then use these estimates to run the <tt>CSSR</tt> algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup_marg, word_lookup_fut = estimate_predictive_distributions(stringX, stringY, L_max_words)\n",
    "\n",
    "epsilon, invepsilon, morph_by_state = run_transCSSR(word_lookup_marg, word_lookup_fut, L_max_CSSR, axs, ays, e_symbols, Xt_name, Yt_name, alpha = alpha, all_digits = True)\n",
    "\n",
    "print 'The epsilon-transducer has {} states.'.format(len(invepsilon))\n",
    "\n",
    "print_morph_by_states(morph_by_state, axs, ays, e_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of <tt>CSSR</tt> is stored in the <tt>transCSSR_results</tt> results directory in dot files, which can be easily viewed using [graphviz](http://graphviz.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source.from_file(transducer_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Tuning Parameters for <tt>CSSR</tt> Using a Train-Test Split of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we set <tt>L_max_CSSR</tt> arbitrarily. A more principled way to choose <tt>L_max_CSSR</tt> is via a [train/test split](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets), where we use half of our data to estimate the $\\epsilon$-machine, and then use the other half of the data to determine how well the estimated $\\epsilon$-machine describes the data. To determine how well the estimated $\\epsilon$-machine describes the data, we use the [log-loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) between the estimated model and the observed testing data. Our goal is to choose the <tt>L_max_CSSR</tt> that **minimizes** the log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stringY_train = stringY[:len(stringY)//2]\n",
    "stringY_test  = stringY[len(stringY)//2:]\n",
    "\n",
    "stringX_train = '0'*len(stringY_train)\n",
    "stringX_test  = '0'*len(stringY_test)\n",
    "\n",
    "ays_lookup = {}\n",
    "y_labels = []\n",
    "\n",
    "for y_ind, y in enumerate(ays):\n",
    "    ays_lookup[y] = y_ind\n",
    "    y_labels.append(y_ind)\n",
    "\n",
    "arrayY = numpy.zeros(len(stringY_test), dtype = 'int16')\n",
    "\n",
    "for t, y in enumerate(stringY_test):\n",
    "    arrayY[t] = ays_lookup[y]\n",
    "\n",
    "word_lookup_marg, word_lookup_fut = estimate_predictive_distributions(stringX_train, stringY_train, L_max_words)\n",
    "\n",
    "log_loss_by_L = []\n",
    "\n",
    "Ls = range(1, L_max_CSSR+1)\n",
    "\n",
    "for L in Ls:\n",
    "    epsilon, invepsilon, morph_by_state = run_transCSSR(word_lookup_marg, word_lookup_fut, L, axs, ays, e_symbols, Xt_name, Yt_name, alpha = alpha, all_digits = True)\n",
    "    \n",
    "    try: # If we attempt to filter a forbidden past, filter_and_pred_probs will throw an error.\n",
    "        pred_probs_by_time, cur_states_by_time = filter_and_pred_probs(stringX_test, stringY_test, machine_fname, transducer_fname, axs, ays, inf_alg)\n",
    "        log_loss_by_L.append(log_loss(y_pred=pred_probs_by_time, y_true=arrayY, labels = y_labels))\n",
    "    except:\n",
    "        log_loss_by_L.append(numpy.nan)\n",
    "    \n",
    "    print('Using L = {}, the Log-Loss is {}.'.format(L, log_loss_by_L[-1]))\n",
    "\n",
    "L_opt = Ls[numpy.nanargmin(log_loss_by_L)]\n",
    "\n",
    "print('Train / Test split with log-loss chooses L_opt = {}'.format(L_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_by_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Ls, log_loss_by_L)\n",
    "plt.xlabel('Maximum Lookback for Reconstruction (L)')\n",
    "plt.ylabel('Log-Loss(L)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we re-estimate the $\\epsilon$-machine using all of the data, with the <tt>L_max_CSSR</tt> chosen by the log-loss minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup_marg, word_lookup_fut = estimate_predictive_distributions(stringX, stringY, L_opt)\n",
    "\n",
    "epsilon, invepsilon, morph_by_state = run_transCSSR(word_lookup_marg, word_lookup_fut, L_opt, axs, ays, e_symbols, Xt_name, Yt_name, alpha = alpha, all_digits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source.from_file(transducer_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Information- and Computation-theoretic Quantities from the $\\epsilon$-Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we investigate some of the information- and computation-theoretic properties of the $\\epsilon$-machine. The algorithms for doing so can be found in:\n",
    "\n",
    "J. P. Crutchfield, C. J. Ellison, and P. M. Riechers, “Exact complexity: The spectral decomposition of intrinsic computation,” Physics Letters A, vol. 380, no. 9, pp. 998–1002, Mar. 2016. [arXiv](https://arxiv.org/abs/1309.3792)\n",
    "\n",
    "In particular, we compute various asymptotic properties:\n",
    "\n",
    "* The statistical complexity $C_{\\mu}$.\n",
    "* The entropy rate $h_{\\mu}$.\n",
    "* The excess entropy $\\mathbf{E}$.\n",
    "\n",
    "and various finite-$L$ properties:\n",
    "\n",
    "* The entropy of words of length $L$: $H[X_{1}^{L}]$\n",
    "* The conditional entropy using words of length $L-1$: $H[X_{L} \\mid X_{1}^{L-1}$]\n",
    "* The excess entropy between finite past-future blocks: $E(L) = I[X_{-L}^{-1} \\wedge X_{0}^{L-1}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_max_ict = 20\n",
    "\n",
    "HLs, hLs, hmu, ELs, E, Cmu, etas_matrix = compute_ict_measures(transducer_fname, ays, inf_alg, L_max = L_max_ict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cmu      = {}\\nH[X_{{0}}] = {}\\nhmu      = {}\\nE        = {}'.format(Cmu, HLs[0], hmu, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(0, L_max_ict), hLs, '.')\n",
    "plt.axhline(hmu, color = 'red', linestyle = '--', label = '$h_{\\mu}$')\n",
    "plt.xlabel('Lookback $L$')\n",
    "plt.ylabel('$H[X_{0} \\mid X_{-L}^{-1}]$')\n",
    "ax = fig.gca(); ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(1, L_max_ict + 1), ELs, '.')\n",
    "plt.axhline(E, color = 'red', linestyle = '--', label = '$\\mathbf{E}$')\n",
    "plt.xlabel('Lookback $L$')\n",
    "plt.ylabel('$E(L) = I[X_{-L}^{-1} \\wedge X_{0}^{L-1}]$')\n",
    "ax = fig.gca(); ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
